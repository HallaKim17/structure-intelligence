import streamlit as st
import os
import json
import networkx as nx
from pyvis.network import Network
from dotenv import load_dotenv
from openai import OpenAI

load_dotenv()
client = OpenAI(api_key=os.getenv("OPENAI_API_KEY"))

st.set_page_config(page_title="Meaning Structure Analyzer", layout="wide")

# --- LLM í•¨ìˆ˜: ì˜ë¯¸ë©ì–´ë¦¬ ì¶”ì¶œ ---
def extract_meaning_chunks(text):
    prompt = f"""
    ë„ˆì˜ ëª©í‘œëŠ” ì£¼ì–´ì§„ í…ìŠ¤íŠ¸ì—ì„œ "ê¸´ì¥ê³¼ í•´ì†Œ", "ë–¡ë°¥ê³¼ íšŒìˆ˜" ê´€ê³„ì— ìˆëŠ” ì˜ë¯¸ë©ì–´ë¦¬(semantic chunks) ìŒë“¤ì„ ì¶”ì¶œí•˜ëŠ” ê±°ì•¼. 
    ê° ì˜ë¯¸ë©ì–´ë¦¬ì— ëŒ€í•´:
      - id (0ë¶€í„° ì •ìˆ˜)
      - summary (1ì¤„) 
      - role (ê¸´ì¥/í•´ì†Œ/ë–¡ë°¥/íšŒìˆ˜ ì¤‘ íƒ1)
      - link (ì—°ê²°ë˜ì–´ ìˆëŠ” ë‹¤ë¥¸ ì˜ë¯¸ë©ì–´ë¦¬ id)

    'í•´ì†Œ' ì˜ë¯¸ë©ì–´ë¦¬ëŠ” ë‹¤ë¥¸ ì˜ë¯¸ë©ì–´ë¦¬ë¥¼ ê°€ë¦¬í‚¤ì§€ ì•Šì•„.
    'íšŒìˆ˜' ì˜ë¯¸ë©ì–´ë¦¬ëŠ” ë‹¤ë¥¸ ì˜ë¯¸ë©ì–´ë¦¬ë¥¼ ê°€ë¦¬í‚¤ì§€ ì•Šì•„.
    'ê¸´ì¥' ì˜ë¯¸ë©ì–´ë¦¬ëŠ” í•˜ë‚˜ ë˜ëŠ” ì—¬ëŸ¬ ê°œì˜ 'í•´ì†Œ' ì˜ë¯¸ë©ì–´ë¦¬ë§Œ ê°€ë¦¬í‚¬ ìˆ˜ ìˆì–´.
    'ë–¡ë°¥' ì˜ë¯¸ë©ì–´ë¦¬ëŠ” í•˜ë‚˜ ë˜ëŠ” ì—¬ëŸ¬ ê°œì˜ 'íšŒìˆ˜' ì˜ë¯¸ë©ì–´ë¦¬ë§Œ ê°€ë¦¬í‚¬ ìˆ˜ ìˆì–´.
    edgeëŠ” 'ê¸´ì¥'--> 'í•´ì†Œ', 'ë–¡ë°¥'-->'íšŒìˆ˜'ì¸ ì—°ê²°ë§Œ ì¡´ì¬í•´. í•˜ë‚˜ì˜ ì˜ë¯¸ë©ì–´ë¦¬ëŠ” ì—¬ëŸ¬ ê°œì˜ roleì„ ê°€ì§ˆ ìˆ˜ ìˆì–´.  ëŒ€ì‹  ê° roleì— ëŒ€í•´ edgeë¡œ ëŒ€ì‘ë˜ëŠ” ì˜ë¯¸ë©ì–´ë¦¬ê°€ ì—°ê²°ë˜ì–´ì•¼ í•´.
    ì˜ë¯¸ë©ì–´ë¦¬ëŠ” ë°˜ë“œì‹œ 2ê°œ ì´ìƒì´ì–´ì•¼ í•´.

    ì¶œë ¥ì€ JSON ë°°ì—´ë¡œë§Œ.
    [
     {{
      "id": 0,
      "summary": "...",
      "role": "...",
      "link": [3]
     }}
    ]
    í…ìŠ¤íŠ¸:
    {text}
    """

    response = client.chat.completions.create(
        #model="o3-2025-04-16",
        model="gpt-4o-mini",
        response_format={"type": "json_object"},
        messages=[{"role": "user", "content": prompt}],
        temperature=0.4
    )

    try:
        print(response.choices[0].message.content)
        return json.loads(response.choices[0].message.content)
    except:
        st.error("âš ï¸ JSON íŒŒì‹±ì— ì‹¤íŒ¨í–ˆìŠµë‹ˆë‹¤. í”„ë¡¬í”„íŠ¸ë¥¼ ì¡°ì •í•˜ì„¸ìš”.")
        return []


# --- pyvisë¡œ ê·¸ë˜í”„ ìƒì„± ---
def create_graph(chunks):
    print(chunks)
    print(type(chunks))
    G = Network(height="600px", width="100%", directed=True)
    G.barnes_hut()
    
    chunks = list(chunks.values())[0]
    # ë…¸ë“œ ì¶”ê°€
    for c in chunks:
        label = f"{c['id']}: {c['summary']}"
        color = "#FFD966" if c['role'] == "ë–¡ë°¥" else \
                "#FF6F61" if c['role'] == "íšŒìˆ˜" else \
                "#6FA8DC" if c['role'] == "ê¸´ì¥" else \
                "#93C47D" if c['role'] == "í•´ì†Œ" else "#CCCCCC"

        G.add_node(c["id"], label=label, color=color)

    # ì—£ì§€ ì¶”ê°€
    for c in chunks:
        if "link" in c:
            for nxt in c["link"]:
                G.add_edge(c["id"], nxt)

    return G


# --- Streamlit UI ---
st.title("ğŸ”® Meaning Chunk Structure Analyzer (MVP)")
st.markdown("**í…ìŠ¤íŠ¸ â†’ ì˜ë¯¸ë©ì–´ë¦¬ â†’ êµ¬ì¡° ê·¸ë˜í”„ â†’ êµ¬ì¡° ë¶„ì„ ë¦¬í¬íŠ¸** ìë™ ìƒì„±")

text_input = st.text_area("âœ¨ ë¶„ì„í•  í…ìŠ¤íŠ¸ë¥¼ ì…ë ¥í•˜ì„¸ìš”", height=200)

if st.button("ì˜ë¯¸ êµ¬ì¡° ë¶„ì„ ì‹œì‘"):
    if not text_input.strip():
        st.warning("í…ìŠ¤íŠ¸ë¥¼ ì…ë ¥í•˜ì„¸ìš”.")
        st.stop()

    with st.spinner("LLMìœ¼ë¡œ ì˜ë¯¸ë©ì–´ë¦¬ ì¶”ì¶œ ì¤‘..."):
        chunks = extract_meaning_chunks(text_input)

    st.subheader("ğŸ“Œ 1. ì˜ë¯¸ë©ì–´ë¦¬(semantic chunk) ì¶”ì¶œ ê²°ê³¼")
    st.json(chunks)

    st.subheader("ğŸ“Œ 2. ì˜ë¯¸ ê´€ê³„ ê·¸ë˜í”„")
    graph = create_graph(chunks)
    graph.save_graph("graph.html")
    st.components.v1.html(open("graph.html", "r").read(), height=650)

    # ì˜ë¯¸ êµ¬ì¡° ìš”ì•½ ìƒì„±
    with st.spinner("êµ¬ì¡° ë¶„ì„ ë³´ê³ ì„œ ìƒì„± ì¤‘..."):
        summary_prompt = f"""
        ì•„ë˜ ì˜ë¯¸ë©ì–´ë¦¬ ë¦¬ìŠ¤íŠ¸ë¥¼ ê¸°ë°˜ìœ¼ë¡œ ì „ì²´ êµ¬ì¡°ì  íŠ¹ì§•ì„ ë¶„ì„í•´ì¤˜.

        - í•µì‹¬ ì˜ë¯¸ë©ì–´ë¦¬ ìš”ì•½
        - ê¸´ì¥-í•´ì†Œ êµ¬ì¡° íŒŒì•…
        - ë–¡ë°¥-íšŒìˆ˜ êµ¬ì¡° ì—¬ë¶€
        - êµ¬ì¡°ì  ë¦¬ìŠ¤í¬(ëª¨ìˆœ, êµ¬ë©, ì—°ê²° ì•½í•¨ ë“±)
        - ì „ì²´ ìŠ¤í† ë¦¬ ì•„í‚¤í…ì²˜ í‰ê°€ (10ì  ì²™ë„)

        ì˜ë¯¸ë©ì–´ë¦¬:
        {chunks}
        """

        summary_resp = client.chat.completions.create(
            model="gpt-4.1-mini",
            messages=[{"role": "user", "content": summary_prompt}],
            temperature=0.3
        ).choices[0].message.content

    st.subheader("ğŸ“Œ 3. êµ¬ì¡° ë¶„ì„ ë¦¬í¬íŠ¸")
    st.write(summary_resp)

